{
	"name": "Prod_Extended_Amortized_Cost-Oneyear",
	"properties": {
		"description": "\n",
		"folder": {
			"name": "NotebookInProduction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "de526ce2-114e-45af-9fc1-7857f7785f4b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This notebook is used to extract all extended amortized cost files from storage account as dataframes and append them into one single dataframe.\r\n",
					"\r\n",
					"The purpose is to make PBI desktop convenient and faster to load the data. \r\n",
					"\r\n",
					"The appended data is considered as a large dataset with more than a hundred million rows and multiple columns. \r\n",
					"\r\n",
					"To reduce the size of the data, a years period with all columns has been selected. This has been verified and confirmed by the end-users (WBS owners).\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import Libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd \r\n",
					"import pyspark.pandas as ps\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.types import StructType\r\n",
					"from pyspark.sql.functions import lit\r\n",
					"from pyspark.sql.functions import year, month\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from datetime import datetime, timedelta\r\n",
					"from pyspark.sql.functions import when\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load the data"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Collect the data schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Identify a years period\r\n",
					"date_one_year_ago = datetime.now().date()-timedelta(days=365)\r\n",
					"\r\n",
					"# Define a path for the data with random period\r\n",
					"path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost/20230501-20230531/Extended_v3_ACMMonthlyAmortizedCost_20230501-20230531.parquet\"\r\n",
					"\r\n",
					"# Load the data schema and add a new column for Report Date\r\n",
					"df_schema_extended = spark.read.format('parquet').load(path)\r\n",
					"\r\n",
					"df_schema_extended = df_schema_extended.withColumn('Report_Date',lit('') )\r\n",
					"df_schema_extended = df_schema_extended.withColumn('Report_Date_Partition',lit('') )"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(\"A-year-ago's date since today:\",date_one_year_ago)\r\n",
					"len(df_schema_extended.columns)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# In case there is a need to change the column name\r\n",
					"\r\n",
					"#df_schema_extended_pd = df_schema_extended_pd.rename(columns={'Azure Hybrid Benefit': 'Azure_Hybrid_Benefit'})\r\n",
					"#df_schema_extended = spark.createDataFrame(df_schema_extended_pd)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load all ACMMonthlyAmortizedCost data and append to one dataframe."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Create a SparkSession\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"\r\n",
					"# Define an empty schema\r\n",
					"schema = df_schema_extended.schema\r\n",
					"\r\n",
					"# Create an empty DataFrame with the specified schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the root path\r\n",
					"root_path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost\"\r\n",
					"\r\n",
					"# Collect all the folders under this root path\r\n",
					"folders = mssparkutils.fs.ls(root_path)\r\n",
					"\r\n",
					"\r\n",
					"for folder in folders:\r\n",
					"    # Collect all file path from the folder\r\n",
					"        datetime_str = folder.name[0:8]\r\n",
					"        year = int(datetime_str[0:4])\r\n",
					"        month = int(datetime_str[4:6])\r\n",
					"        day = int(datetime_str[6:8])\r\n",
					"        timestamp = pd.Timestamp(year=year, month=month, day=day).date()\r\n",
					"        if timestamp > date_one_year_ago:\r\n",
					"            file_paths = mssparkutils.fs.ls(root_path +'/'+ str(folder.name))\r\n",
					"            for file_path in file_paths:\r\n",
					"                # Focus on parquet file not csv file\r\n",
					"                if 'Extended_v3' in file_path.name:\r\n",
					"                    try:\r\n",
					"                        # Add a Report date column in the dataframe\r\n",
					"                        date = pd.to_datetime(folder.path.split('/')[-1].split('-')[0],format=\"%Y/%m/%d\")\r\n",
					"                        df_temp = spark.read.format('parquet').load(file_path.path)\r\n",
					"                        print(date)\r\n",
					"                        print(\"Filepath:\", file_path.path)\r\n",
					"                        df_temp = df_temp.withColumn('Report_Date', lit(date))\r\n",
					"                        df_temp = df_temp.withColumn('Report_Date_Partition', lit(date))\r\n",
					"                        # Append the dataframe\r\n",
					"                        df = df.union(df_temp)\r\n",
					"                    except Exception as e:\r\n",
					"                        # Exclude the extended files that are developed by David.\r\n",
					"                        print(\"Those files will not be read\",file_path.path)\r\n",
					"                        print(df_temp.columns)\r\n",
					"                        print(df)\r\n",
					"                        print(len(df_temp.columns),len(df.columns))\r\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Display the dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Change the data type of the 'Report_Date' column to Datetime\r\n",
					"df = df.withColumn(\"Report_Date\", col(\"Report_Date\").cast(\"timestamp\"))"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Change the column name\r\n",
					"df = df.withColumnRenamed(\"Application_Name\", \"ApplicationName\")\r\n",
					"\r\n",
					"# Applied Unknown label in ApplicationName field. \r\n",
					"condition_1 = (F.col('ApplicationName').isNull()| (F.col('ApplicationName') == \"\")) & (F.col('SubscriptionName').contains('Danske Commodities'))\r\n",
					"condition_2 = (F.col('ApplicationName').isNull()| (F.col('ApplicationName') == \"\")) & (~(F.col('SubscriptionName').contains('Danske Commodities')))\r\n",
					"\r\n",
					"df = df.withColumn(\r\n",
					"    'ApplicationName',\r\n",
					"    F.when(condition_1,'Danske Commodities'\r\n",
					"    ).when(condition_2,'Unknown'\r\n",
					"    ).otherwise(F.col('ApplicationName'))\r\n",
					")\r\n",
					""
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# print('Number of rows in the appended dataframe:',df.count())\r\n",
					"# df.printSchema()\r\n",
					"#display(df.filter((F.col('ApplicationName')).contains('Danske Commodities')))\r\n",
					"#display(df.filter((F.col('ApplicationName')).contains('Unknown')))"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Save it to optimized container"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define optimized path to save the data\r\n",
					"optimized_path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_OneYear.parquet\" \r\n",
					"\r\n",
					"# Save the data with partitioned by Report date\r\n",
					"df.write.format('parquet').mode('overwrite').partitionBy('Report_Date_Partition').option('overwriteSchema', 'true').save(optimized_path)"
				],
				"execution_count": 11
			}
		]
	}
}