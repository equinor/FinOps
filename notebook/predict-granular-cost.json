{
	"name": "predict-granular-cost",
	"properties": {
		"folder": {
			"name": "NotebookInProduction/Cost Prediction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "242d5c10-7bec-43c6-b8fc-5aa29aa499b2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Initialize script"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"from pyspark.ml.regression import LinearRegression\r\n",
					"from pyspark.ml.regression import GeneralizedLinearRegression\r\n",
					"from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, FeatureHasher, Bucketizer\r\n",
					"from pyspark.ml import Pipeline\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from datetime import date, timedelta, datetime\r\n",
					"from dateutil.relativedelta import relativedelta\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"import itertools\r\n",
					"import json"
				],
				"execution_count": 78
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load and aggregate usage data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cost_columns = [\r\n",
					"    'SubscriptionName',\r\n",
					"    'ResourceGroup'\r\n",
					"    # 'MeterCategory'\r\n",
					"    # 'ActiveWBS'\r\n",
					"]"
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load usage source\r\n",
					"cost_path = f'abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyActualCost/*/Extended_v3_ACMMonthlyActualCost_*.parquet'\r\n",
					"cost_df = spark.read.format('parquet').load(cost_path)\r\n",
					"\r\n",
					"initial_date = datetime.now().date() - relativedelta(months=6)\r\n",
					"cost_df = cost_df.where(F.col('Date') >= initial_date)\r\n",
					"\r\n",
					"# Select appropriate columns\r\n",
					"cost_df = cost_df.select('Date', *cost_columns, 'CostInBillingCurrency')\r\n",
					"cost_df = cost_df.withColumn('Date', F.date_trunc('month', 'Date'))\r\n",
					"cost_df = cost_df.groupBy('Date', *cost_columns).agg(F.sum('CostInBillingCurrency').alias('Cost')).orderBy('Date')\r\n",
					"\r\n",
					"# Filter away latest month - as we predict cost per month, it will mess up future predictions\r\n",
					"cost_df = cost_df.filter(F.col('Date') < F.concat(F.date_format(F.current_date(), 'yyyy'), F.lit('-'), F.date_format(F.current_date(), 'MM'), F.lit('-'), F.lit('01')))\r\n",
					"\r\n",
					"# Only consider positive cost\r\n",
					"cost_df = cost_df.where(F.col('Cost') >= 0)\r\n",
					"\r\n",
					"# Replace null-values with string value\r\n",
					"for col_name in cost_columns:\r\n",
					"    cost_df = cost_df.withColumn(col_name, F.coalesce(F.col(col_name), F.lit('empty')))"
				],
				"execution_count": 86
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"concat_expr = F.reduce(\r\n",
					"      [F.concat(F.col(col_name), F.lit('-')) for col_name in cost_columns]\r\n",
					"  )\r\n",
					"\r\n",
					"cost_df = cost_df.withColumn('CostJoinKey', F.expr(concat_expr[:-len(separator)]))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(test)"
				],
				"execution_count": 83
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Pre-process usage data (zero-fill missing rows)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(len(dimensions))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Construct zero-fill dataframe\r\n",
					"dates = [row.Date for row in cost_df.select('Date').distinct().collect()]\r\n",
					"\r\n",
					"def build_json(row, columns):\r\n",
					"    json_object = {}\r\n",
					"    for col in columns:\r\n",
					"        json_object.update({\r\n",
					"            col: row[col]\r\n",
					"        })\r\n",
					"    return json_object\r\n",
					"\r\n",
					"dimensions = [json.dumps(build_json(row, cost_columns)) for row in cost_df.select(*cost_columns).distinct().collect()]\r\n",
					"\r\n",
					"fill_data = list(itertools.product(dates, dimensions))\r\n",
					"fill_df = spark.createDataFrame(fill_data, [\"Date\", \"Dimensions\"])\r\n",
					"fill_df = fill_df.withColumn('json', F.from_json('Dimensions', 'map<string,string>', options={'inferSchema': 'true'})).drop('Dimensions')\r\n",
					"\r\n",
					"for col_name in cost_columns:\r\n",
					"    fill_df = fill_df.withColumn(col_name, F.col(f'json.{col_name}'))\r\n",
					"\r\n",
					"fill_df = fill_df.drop('json')\r\n",
					"\r\n",
					"fill_df = fill_df.subtract(cost_df.select(\"Date\", *cost_columns))\r\n",
					"fill_df = fill_df.withColumn('Cost', F.lit(0))\r\n",
					"\r\n",
					"# Zero-fill missing rows where resource doesn't have cost\r\n",
					"cost_df = cost_df.union(fill_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Construct feature columns"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# The cost of the previous row will be the main feature for the ML model\r\n",
					"windowSpec = Window.partitionBy(*cost_columns).orderBy(\"Date\")\r\n",
					"cost_df = cost_df.withColumn(\"PrevCost\", F.lag(\"Cost\", 1).over(windowSpec))\r\n",
					"\r\n",
					"# Decompose the date into a integer columns for year and month (ML model will only handle number features)\r\n",
					"cost_df = cost_df.withColumn(\"Year\", F.year(\"Date\"))\r\n",
					"cost_df = cost_df.withColumn(\"Month\", F.month(\"Date\"))\r\n",
					"\r\n",
					"# Remove null values\r\n",
					"cost_df = cost_df.dropna(subset=[\"PrevCost\"])\r\n",
					"\r\n",
					"# Add an additional bucket feature indicating the cost level (important to capture the dynamics of increasing vs diminishing costs)\r\n",
					"# Create logarithmic scale bucket ranges\r\n",
					"bucketizer_splits = [0] + [10 ** x for x in range(0, 6)] + [float(\"inf\")]\r\n",
					"\r\n",
					"# Apply bucketizer to dataframe to create cost bin feature column\r\n",
					"bucketizer = Bucketizer(splits=bucketizer_splits, inputCol=\"PrevCost\", outputCol=\"CostBin\")\r\n",
					"cost_df = bucketizer.transform(cost_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Build linear regression model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Index and encode string features - prerequisite for fitting model\r\n",
					"# subscription_indexer = StringIndexer(inputCol=\"SubscriptionName\", outputCol=\"SubscriptionIndex\")\r\n",
					"# subscription_encoder = OneHotEncoder(inputCol=\"SubscriptionIndex\", outputCol=\"SubscriptionVec\")\r\n",
					"\r\n",
					"# rg_indexer = StringIndexer(inputCol=\"ResourceGroup\", outputCol=\"ResourceGroupIndex\")\r\n",
					"# rg_encoder = OneHotEncoder(inputCol=\"ResourceGroupIndex\", outputCol=\"ResourceGroupVec\")\r\n",
					"\r\n",
					"# Create structure of input and output values for model\r\n",
					"assembler = VectorAssembler(inputCols=[\"Year\", \"Month\", \"PrevCost\", \"CostBin\"], outputCol=\"features\")\r\n",
					"\r\n",
					"# Define the linear regression model parameters and pipeline configuration\r\n",
					"glr = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"Cost\", family=\"gaussian\", link=\"identity\")\r\n",
					"pipeline = Pipeline(stages=[assembler, glr])\r\n",
					"\r\n",
					"# Build linear regression model\r\n",
					"pipeline_model = pipeline.fit(cost_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Preparing dataframe structure for prediction"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define and build the prediction horizon length in the number of months \r\n",
					"prediction_interval = 12\r\n",
					"last_date = last_date = max(dates)\r\n",
					"future_dates = [last_date + relativedelta(months=i) for i in range(1, prediction_interval + 1)]\r\n",
					"\r\n",
					"# Create new dataframe structure containing all possible combinations of features and dates\r\n",
					"future_data = list(itertools.product(future_dates, dimensions))\r\n",
					"future_df = spark.createDataFrame(future_data, [\"Date\", \"Dimensions\"])\r\n",
					"future_df = future_df.withColumn('json', F.from_json('Dimensions', 'map<string,string>', options={'inferSchema': 'true'})).drop('Dimensions')\r\n",
					"\r\n",
					"for col_name in cost_columns:\r\n",
					"    future_df = future_df.withColumn(col_name, F.col(f'json.{col_name}'))\r\n",
					"\r\n",
					"future_df = future_df .drop('json')\r\n",
					"\r\n",
					"# Split date into months and years to match structure of the original dataFrame\r\n",
					"future_df = future_df.withColumn(\"Year\", F.year(\"Date\"))\r\n",
					"future_df = future_df.withColumn(\"Month\", F.month(\"Date\"))\r\n",
					"\r\n",
					"# Find the latest cost for each feature set\r\n",
					"latest_cost_window_spec = Window.partitionBy(*cost_columns).orderBy(F.desc(\"Date\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\r\n",
					"\r\n",
					"latest_cost_df = cost_df\\\r\n",
					"    .select('Date', *cost_columns, 'Cost')\\\r\n",
					"    .withColumn(\"rank\", F.row_number().over(latest_cost_window_spec)) \\\r\n",
					"    .where(F.col('rank') == 1) \\\r\n",
					"    .drop('Date', 'rank')\\\r\n",
					"    .withColumnRenamed('Cost', 'PrevCost')\r\n",
					"\r\n",
					"# Add prefix to cost columns to prepare dataframe for join operation\r\n",
					"for col_name in cost_columns:\r\n",
					"    latest_cost_df = latest_cost_df.withColumnRenamed(col_name, f'temp_{col_name}')\r\n",
					"\r\n",
					"latest_cost_df = bucketizer.transform(latest_cost_df)\r\n",
					"\r\n",
					"# # The cost value for each feature set (and for all dates) will be given by the latest known cost for that feature set\r\n",
					"future_df = future_df.join(latest_cost_df, (future_df.SubscriptionName == latest_cost_df.temp_SubscriptionName) & (F.coalesce(future_df.ResourceGroup, F.lit('')) == F.coalesce(latest_cost_df.temp_ResourceGroup, F.lit(''))), 'left')\r\n",
					"\r\n",
					"# Remove all temp columns\r\n",
					"future_df = future_df.drop(*['temp_' + col_name for col_name in cost_columns])"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Forecast cost using linear regression model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"future_predictions = pipeline_model.transform(future_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"post = cost_df.select('Date', *cost_columns, 'Cost', 'CostBin')\r\n",
					"pred = future_predictions.select('Date', *cost_columns, 'prediction', 'CostBin').withColumnRenamed('prediction', 'Cost')\r\n",
					"total = post.union(pred)\r\n",
					"total = total.withColumn('Cost', F.when(F.col('Cost') < 0, F.lit(0)).otherwise(F.col('Cost')))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(total)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}