{
	"name": "predict-granular-cost",
	"properties": {
		"folder": {
			"name": "NotebookInProduction/Cost Prediction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "64666047-227b-45d2-ae28-e0704ca7fea9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Initialize script"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"import pyspark.sql.types as T\r\n",
					"from pyspark.ml.regression import LinearRegression\r\n",
					"from pyspark.ml.regression import GeneralizedLinearRegression\r\n",
					"from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, FeatureHasher, Bucketizer\r\n",
					"from pyspark.ml import Pipeline\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from datetime import date, timedelta, datetime\r\n",
					"from dateutil.relativedelta import relativedelta\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"import itertools\r\n",
					"import json"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"LOOKBACK_PERIOD = 24 # Number of months\r\n",
					"PREDICTION_INTERVAL = 12 # Number of months"
				],
				"execution_count": 60
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load and aggregate usage data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cost_columns = [\r\n",
					"    'SubscriptionName',\r\n",
					"    'ResourceGroup',\r\n",
					"    'MeterCategory',\r\n",
					"    'ActiveWBS',\r\n",
					"    'ResourceName'\r\n",
					"]"
				],
				"execution_count": 61
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load usage source\r\n",
					"cost_path = f'abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyActualCost/*/Extended_v3_ACMMonthlyActualCost_*.parquet'\r\n",
					"cost_df = spark.read.format('parquet').load(cost_path)\r\n",
					"\r\n",
					"initial_date = datetime.now().date() - relativedelta(months=LOOKBACK_PERIOD)\r\n",
					"cost_df = cost_df.where(F.col('Date') >= initial_date)\r\n",
					"\r\n",
					"# Select appropriate columns\r\n",
					"cost_df = cost_df.select('Date', *cost_columns, 'CostInBillingCurrency')\r\n",
					"cost_df = cost_df.withColumn('Date', F.date_trunc('month', 'Date'))\r\n",
					"cost_df = cost_df.groupBy('Date', *cost_columns).agg(F.sum('CostInBillingCurrency').alias('Cost')).orderBy('Date')\r\n",
					"\r\n",
					"# Filter away latest month - as we predict cost per month, it will mess up future predictions\r\n",
					"cost_df = cost_df.filter(F.col('Date') < F.concat(F.date_format(F.current_date(), 'yyyy'), F.lit('-'), F.date_format(F.current_date(), 'MM'), F.lit('-'), F.lit('01')))\r\n",
					"\r\n",
					"# Only consider positive cost\r\n",
					"cost_df = cost_df.where(F.col('Cost') >= 0)\r\n",
					"\r\n",
					"# Replace null-values with string value\r\n",
					"for col_name in cost_columns:\r\n",
					"    cost_df = cost_df.withColumn(col_name, F.coalesce(F.col(col_name), F.lit('empty')))\r\n",
					"\r\n",
					"generate_join_key = F.concat(*[F.concat(F.col(col_name), F.lit('.')) for col_name in cost_columns])\r\n",
					"cost_df = cost_df.withColumn('CostJoinKey', generate_join_key)"
				],
				"execution_count": 62
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Pre-process usage data (zero-fill missing rows)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Construct zero-fill dataframe\r\n",
					"dates_df = cost_df.select('Date').distinct()\r\n",
					"dimensions_df = cost_df.select(*cost_columns).distinct()\r\n",
					"fill_df = dates_df.crossJoin(dimensions_df)\r\n",
					"\r\n",
					"# Don't zero-fill for dates where we already have cost\r\n",
					"fill_df = fill_df.subtract(cost_df.select(\"Date\", *cost_columns))\r\n",
					"fill_df = fill_df.withColumn('Cost', F.lit(0))\r\n",
					"\r\n",
					"fill_df = fill_df.withColumn('CostJoinKey', generate_join_key)\r\n",
					"\r\n",
					"# Zero-fill missing rows where resource doesn't have cost\r\n",
					"cost_df = cost_df.union(fill_df)"
				],
				"execution_count": 63
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Construct feature columns"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"feature_columns = [\r\n",
					"    'Year',\r\n",
					"    'Month',\r\n",
					"    'PrevCost',\r\n",
					"    'CostBin',\r\n",
					"    'CostDiff'\r\n",
					"]"
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"window_ordered = Window.partitionBy('CostJoinKey').orderBy(\"Date\")\r\n",
					"window_unordered = Window.partitionBy('CostJoinKey')\r\n",
					"\r\n",
					"# Ignore resources where cost is 0 in entire date range\r\n",
					"cost_df = cost_df.withColumn(\"WindowSum\", F.sum(\"Cost\").over(window_unordered))\r\n",
					"cost_df = cost_df.where(F.col(\"WindowSum\") > 0).drop(\"WindowSum\")\r\n",
					"\r\n",
					"# Get difference in latest and earliest cost\r\n",
					"cost_df = cost_df.withColumn(\"LatestCost\", F.last(\"Cost\").over(window_unordered))\r\n",
					"cost_df = cost_df.withColumn(\"EarliestCost\", F.first(\"Cost\").over(window_unordered))\r\n",
					"cost_df = cost_df.withColumn(\"CostDiff\", F.col(\"LatestCost\") - F.col(\"EarliestCost\")).drop(\"LatestCost\", \"EarliestCost\")\r\n",
					"\r\n",
					"# The cost of the previous row will be the main feature for the ML model\r\n",
					"cost_df = cost_df.withColumn(\"PrevCost\", F.lag(\"Cost\", 1).over(window_ordered))\r\n",
					"\r\n",
					"# Decompose the date into a integer columns for year and month (ML model will only handle number features)\r\n",
					"cost_df = cost_df.withColumn(\"Year\", F.year(\"Date\"))\r\n",
					"cost_df = cost_df.withColumn(\"Month\", F.month(\"Date\"))\r\n",
					"\r\n",
					"# Remove null values\r\n",
					"cost_df = cost_df.dropna(subset=[\"PrevCost\", \"CostDiff\"])\r\n",
					"\r\n",
					"# Add an additional bucket feature indicating the cost level (important to capture the dynamics of increasing vs diminishing costs)\r\n",
					"# Create logarithmic scale bucket ranges\r\n",
					"bucketizer_splits = [0] + [10 ** x for x in range(0, 6)] + [float(\"inf\")]\r\n",
					"\r\n",
					"# Apply bucketizer to dataframe to create cost bin feature column\r\n",
					"bucketizer = Bucketizer(splits=bucketizer_splits, inputCol=\"PrevCost\", outputCol=\"CostBin\")\r\n",
					"cost_df = bucketizer.transform(cost_df)"
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Build linear regression model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create structure of input and output values for model\r\n",
					"assembler = VectorAssembler(inputCols=[\"Year\", \"Month\", \"PrevCost\", \"CostBin\"], outputCol=\"features\")\r\n",
					"\r\n",
					"# Define the linear regression model parameters and pipeline configuration\r\n",
					"glr = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"Cost\", family=\"gaussian\", link=\"identity\")\r\n",
					"pipeline = Pipeline(stages=[assembler, glr])\r\n",
					"\r\n",
					"# Build linear regression model\r\n",
					"pipeline_model = pipeline.fit(cost_df)"
				],
				"execution_count": 66
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Preparing dataframe structure for prediction"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define and build the prediction horizon length in the number of months \r\n",
					"last_date = dates_df.agg(F.max('Date')).collect()[0][0]\r\n",
					"future_dates = [last_date + relativedelta(months=i) for i in range(1, PREDICTION_INTERVAL + 1)]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"latest_cost_window_spec = Window.partitionBy(*cost_columns).orderBy(F.desc(\"Date\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\r\n",
					"result = cost_df.select('Date', *cost_columns, *feature_columns, 'Cost', 'CostJoinKey')\r\n",
					"latest_cost_df = result \\\r\n",
					"    .withColumn(\"rank\", F.row_number().over(latest_cost_window_spec)) \\\r\n",
					"    .where(F.col('rank') == 1) \\\r\n",
					"    .drop('Date', 'rank')\\\r\n",
					"    .withColumn('PrevCost', F.col('Cost'))\r\n",
					"\r\n",
					"latest_2 = result.groupBy(*cost_columns).agg(F.max('Date').alias('Date'), F.first('Cost').alias('Cost'))\r\n",
					""
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Find the latest cost for each feature set\r\n",
					"latest_cost_window_spec = Window.partitionBy(*cost_columns).orderBy(F.desc(\"Date\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\r\n",
					"result = cost_df.select('Date', *cost_columns, *feature_columns, 'Cost', 'CostJoinKey')\r\n",
					"\r\n",
					"# latest_cost_df = result \\\r\n",
					"#     .withColumn(\"rank\", F.row_number().over(latest_cost_window_spec)) \\\r\n",
					"#     .where(F.col('rank') == 1) \\\r\n",
					"#     .drop('Date', 'rank') \\\r\n",
					"#     .withColumn('PrevCost', F.col('Cost')) \\\r\n",
					"#     .select('CostJoinKey', 'PrevCost', 'CostDiff')\r\n",
					"\r\n",
					"# latest_cost_df = bucketizer.transform(latest_cost_df)\r\n",
					"\r\n",
					"# current_df = current_df.join(latest_cost_df, 'CostJoinKey', 'left')\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"for current_date in future_dates:\r\n",
					"    print(f'Start predicting for {current_date}')\r\n",
					"    current_df = spark.createDataFrame([(d,) for d in [current_date]], [\"Date\"]).withColumn(\"Date\", F.col(\"Date\").cast(T.DateType()))\r\n",
					"\r\n",
					"    # Create new dataframe structure containing all possible combinations of features and dates\r\n",
					"    current_df = current_df.crossJoin(dimensions_df)\r\n",
					"\r\n",
					"    # Split date into months and years to match structure of the original dataFrame\r\n",
					"    current_df = current_df.withColumn(\"Year\", F.year(\"Date\"))\r\n",
					"    current_df = current_df.withColumn(\"Month\", F.month(\"Date\"))\r\n",
					"    current_df = current_df.withColumn('CostJoinKey', generate_join_key)\r\n",
					"\r\n",
					"    latest_cost_df = result \\\r\n",
					"        .withColumn(\"rank\", F.row_number().over(latest_cost_window_spec)) \\\r\n",
					"        .where(F.col('rank') == 1) \\\r\n",
					"        .drop('Date', 'rank') \\\r\n",
					"        .withColumn('PrevCost', F.col('Cost')) \\\r\n",
					"        .select('CostJoinKey', 'PrevCost', 'CostDiff')\r\n",
					"\r\n",
					"    latest_cost_df = bucketizer.transform(latest_cost_df)\r\n",
					"\r\n",
					"    # # The cost value for each feature set (and for all dates) will be given by the latest known cost for that feature set\r\n",
					"    current_df = current_df.join(latest_cost_df, 'CostJoinKey', 'left')\r\n",
					"\r\n",
					"    prediction_df = pipeline_model.transform(current_df)\r\n",
					"    prediction_df = prediction_df.withColumnRenamed('prediction', 'Cost')\r\n",
					"    prediction_df = prediction_df.select('Date', *cost_columns, *feature_columns, 'Cost', 'CostJoinKey')\r\n",
					"    prev_prediction_df = prediction_df.alias('prediction_copy')\r\n",
					"\r\n",
					"    display(prev_prediction_df)\r\n",
					"\r\n",
					"    result = result.union(prediction_df)"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"ordered_cost_df = cost_df.select('Date', *cost_columns, 'Cost', 'Year', 'Month')\r\n",
					"ordered_prediction_df = prediction_df.select('Date', *cost_columns, 'Cost', 'Year', 'Month')\r\n",
					"combined_df = ordered_cost_df.union(ordered_prediction_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define and build the prediction horizon length in the number of months \r\n",
					"last_date = dates_df.agg(F.max('Date')).collect()[0][0]\r\n",
					"future_dates = [last_date + relativedelta(months=i) for i in range(1, PREDICTION_INTERVAL + 1)]\r\n",
					"future_dates_df = spark.createDataFrame([(d,) for d in future_dates], [\"Date\"])\r\n",
					"future_dates_df = future_dates_df.withColumn(\"Date\", F.col(\"Date\").cast(T.DateType()))\r\n",
					"\r\n",
					"# Create new dataframe structure containing all possible combinations of features and dates\r\n",
					"future_df = future_dates_df.crossJoin(dimensions_df)\r\n",
					"\r\n",
					"# Split date into months and years to match structure of the original dataFrame\r\n",
					"future_df = future_df.withColumn(\"Year\", F.year(\"Date\"))\r\n",
					"future_df = future_df.withColumn(\"Month\", F.month(\"Date\"))\r\n",
					"\r\n",
					"future_df = future_df.withColumn('CostJoinKey', generate_join_key)\r\n",
					"\r\n",
					"# Find the latest cost for each feature set\r\n",
					"latest_cost_window_spec = Window.partitionBy(*cost_columns).orderBy(F.desc(\"Date\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\r\n",
					"\r\n",
					"latest_cost_df = cost_df \\\r\n",
					"    .select('Date', *cost_columns, 'Cost', 'CostDiff') \\\r\n",
					"    .withColumn(\"rank\", F.row_number().over(latest_cost_window_spec)) \\\r\n",
					"    .where(F.col('rank') == 1) \\\r\n",
					"    .drop('Date', 'rank') \\\r\n",
					"    .withColumnRenamed('Cost', 'PrevCost')\r\n",
					"\r\n",
					"latest_cost_df = latest_cost_df.withColumn('CostJoinKey', generate_join_key)\r\n",
					"latest_cost_df = latest_cost_df.drop(*cost_columns)\r\n",
					"\r\n",
					"latest_cost_df = bucketizer.transform(latest_cost_df)\r\n",
					"\r\n",
					"# # The cost value for each feature set (and for all dates) will be given by the latest known cost for that feature set\r\n",
					"future_df = future_df.join(latest_cost_df, 'CostJoinKey', 'left')"
				],
				"execution_count": 34
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Forecast cost using linear regression model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"prediction_df = pipeline_model.transform(future_df)\r\n",
					"prediction_df = prediction_df.withColumnRenamed('prediction', 'Cost')"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"ordered_cost_df = cost_df.select('Date', *cost_columns, 'Cost', 'Year', 'Month')\r\n",
					"ordered_prediction_df = prediction_df.select('Date', *cost_columns, 'Cost', 'Year', 'Month')\r\n",
					"combined_df = ordered_cost_df.union(ordered_prediction_df)\r\n",
					"\r\n",
					"# Remove zero and negative cost to reduce data volume\r\n",
					"combined_df = combined_df.where(F.col('Cost') > 0)"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(combined_df.groupBy('Date').agg(F.sum('Cost')))"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write predicted cost to file\r\n",
					"# target_path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/forecasts/granular-cost-{LOOKBACK_PERIOD}-month-lookback\"\r\n",
					"# combined_df.write.format('parquet').mode('overwrite').partitionBy('Year','Month').option('overwriteSchema', 'true').save(target_path)"
				],
				"execution_count": null
			}
		]
	}
}