{
	"name": "predict-service-cost",
	"properties": {
		"folder": {
			"name": "NotebookInProduction/Cost Prediction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "210ce1a4-b237-4a32-9ec6-90f2f0117e06"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 125
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"import pyspark.sql.types as T\r\n",
					"import pyspark.sql.window as W\r\n",
					"import statsmodels.api as sm\r\n",
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"from datetime import datetime, timedelta\r\n",
					"import time\r\n",
					"\r\n",
					"import warnings\r\n",
					"warnings.filterwarnings(\"ignore\", category=FutureWarning)"
				],
				"execution_count": 126
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Integers represent number of months\r\n",
					"PREDICTION_END_DATE = '2029-04-01'\r\n",
					"LOOKBACK = 6"
				],
				"execution_count": 127
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load usage source\r\n",
					"cost_path = f'abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost/*/Extended_v3_ACMMonthlyAmortizedCost_*.parquet'\r\n",
					"cost_df = spark.read.format('parquet').load(cost_path)"
				],
				"execution_count": 128
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"today = datetime.today().replace(day=1)\r\n",
					"\r\n",
					"prediction_end_date = datetime.strptime(PREDICTION_END_DATE, '%Y-%m-%d')\r\n",
					"prediction_interval = (prediction_end_date.year - today.year) * 12 + (prediction_end_date.month - today.month)\r\n",
					"\r\n",
					"lookback_date = (today - timedelta(days=LOOKBACK*30)).replace(day=1)\r\n",
					"lookback_date_formatted = lookback_date.strftime('%Y-%m-%d')\r\n",
					"lookback_diff = (today.year - lookback_date.year) * 12 + (today.month - lookback_date.month)"
				],
				"execution_count": 129
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Select appropriate columns\r\n",
					"cost_df = cost_df.select('Date', 'CostInBillingCurrency', 'MeterCategory')\r\n",
					"cost_df = cost_df.withColumn('Date', F.date_trunc('month', 'Date'))\r\n",
					"cost_df = cost_df.groupBy('Date', 'MeterCategory').agg(F.sum('CostInBillingCurrency').alias('Cost')).orderBy('Date')\r\n",
					"\r\n",
					"# Filter away latest month - as we predict cost per month, it will mess up future predictions\r\n",
					"cost_df = cost_df.filter(F.col('Date') < F.concat(F.date_format(F.current_date(), 'yyyy'), F.lit('-'), F.date_format(F.current_date(), 'MM'), F.lit('-'), F.lit('01')))"
				],
				"execution_count": 130
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create window for iterating over meter categories\r\n",
					"window = W.Window.partitionBy(\"MeterCategory\")\r\n",
					"\r\n",
					"# Only look at data within the lookback period\r\n",
					"count_df = cost_df.where((F.col('Cost') > 0) & (F.col('Date') >= lookback_date_formatted))\r\n",
					"\r\n",
					"# Calculate the number of month between cost date and today\r\n",
					"count_df = count_df.withColumn('months_from_today', F.months_between(F.lit(today), 'Date'))\r\n",
					"\r\n",
					"# Find the lowest month difference across each meter category\r\n",
					"count_df = count_df.withColumn('min_month_diff', F.min('months_from_today').over(window))\r\n",
					"\r\n",
					"# Keep meter categories where lowest diff is 1 month - this means it has cost last month - candidate for forecasting\r\n",
					"count_df = count_df.withColumn('keep_service', F.when(F.col('min_month_diff') == 1, True).otherwise(False))\r\n",
					"\r\n",
					"# Remove duplicates and prepare for join operation\r\n",
					"count_df = count_df.select('MeterCategory', 'keep_service').dropDuplicates(subset=[\"MeterCategory\", \"keep_service\"])"
				],
				"execution_count": 131
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Counting the meter category groups to check if categories have cost in entire time period\r\n",
					"# count_df = cost_df.filter((F.col('Cost') > 0) & (F.col('Date') >= lookback_date_formatted)).groupBy(\"MeterCategory\").count()\r\n",
					"\r\n",
					"# # Only keeping meter categories with cost in every month\r\n",
					"# cost_df = cost_df.join(count_df, \"MeterCategory\").filter(count_df[\"count\"] == lookback_diff).drop('count')"
				],
				"execution_count": 132
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cost_df = cost_df.join(count_df, 'MeterCategory').where(F.col('keep_service') == True).drop('keep_service')"
				],
				"execution_count": 133
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create schema for resulting df\r\n",
					"schema = T.StructType([\r\n",
					"    T.StructField(\"Date\", T.TimestampType(), True),\r\n",
					"    T.StructField(\"Cost\", T.DoubleType(), True),\r\n",
					"    T.StructField(\"MeterCategory\", T.StringType(), True)\r\n",
					"])\r\n",
					"\r\n",
					"df = spark.createDataFrame([], schema)"
				],
				"execution_count": 134
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"distinct_meter_categories = [row[0] for row in cost_df.select('MeterCategory').distinct().collect()]"
				],
				"execution_count": 135
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"for meter_category in distinct_meter_categories:\r\n",
					"    print(f'Processing {meter_category} cost')\r\n",
					"    start_time = time.time()\r\n",
					"    temp_df = cost_df.where(F.col('MeterCategory') == meter_category).orderBy(F.asc('Date')).select('Date', 'Cost', 'MeterCategory')\r\n",
					"\r\n",
					"    # Convert pyspark df to pandas for OLS model\r\n",
					"    temp_pdf = temp_df.toPandas()\r\n",
					"    temp_pdf.set_index(\"Date\", inplace=True)\r\n",
					"\r\n",
					"    temp_pdf['x'] = range(len(temp_pdf))\r\n",
					"\r\n",
					"    # Estimate OLS model\r\n",
					"    if LOOKBACK > len(temp_pdf['Cost']):\r\n",
					"        y = temp_pdf['Cost'].values\r\n",
					"        x = temp_pdf['x'].values\r\n",
					"    else:\r\n",
					"        y = temp_pdf['Cost'].tail(LOOKBACK)\r\n",
					"        x = temp_pdf['x'].tail(LOOKBACK)\r\n",
					"        \r\n",
					"    model = sm.OLS(y, sm.add_constant(x))\r\n",
					"    result = model.fit()\r\n",
					"\r\n",
					"    # Configure prediction period\r\n",
					"    future_months = pd.date_range(start=temp_pdf.index[-1], periods=prediction_interval, freq=\"MS\")[1:]\r\n",
					"    future_x = np.arange(temp_pdf['x'][-1] + 1, temp_pdf['x'][-1] + prediction_interval)\r\n",
					"\r\n",
					"    # Predict future cost\r\n",
					"    x = sm.add_constant(future_x)\r\n",
					"    predicted_cost = result.predict(x)\r\n",
					"\r\n",
					"    predicted_df = pd.DataFrame({\"Date\": future_months, \"Cost\": predicted_cost})\r\n",
					"    predicted_df = spark.createDataFrame(predicted_df).withColumn('MeterCategory', F.lit(meter_category))\r\n",
					"    updated_df = temp_df.union(predicted_df)\r\n",
					"    df = df.union(updated_df)\r\n",
					"    end_time = time.time()\r\n",
					"    print(f'Successfully processed {meter_category} cost in {round(end_time - start_time, 2)} seconds.')"
				],
				"execution_count": 136
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set predicted Cost to 0 if negative\r\n",
					"df = df.withColumn('Cost', F.when(F.col('Cost') >= 0, F.col('Cost')).otherwise(0))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"target_path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/service-cost-prediction.parquet\"\r\n",
					"df.write.format('parquet').mode('overwrite').option('overwriteSchema', 'true').save(target_path)"
				],
				"execution_count": null
			}
		]
	}
}