{
	"name": "AmortizedCostWithBusinessAreaLevel",
	"properties": {
		"folder": {
			"name": "NotebookNotInUse/Delete"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a9d997e8-8f4f-4539-af2a-e67031819931"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd \r\n",
					"import pyspark.pandas as ps\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.types import StructType\r\n",
					"from pyspark.sql.functions import lit\r\n",
					"from pyspark.sql.functions import year, month\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from datetime import datetime, timedelta"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"path=\"abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_OneYear.parquet/Report_Date*/*.parquet\" \r\n",
					"pd.read_parquet(path)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyarrow.parquet as pq\r\n",
					"import os\r\n",
					"\r\n",
					"# Replace 'your_directory' with the path to the directory containing the partitioned Parquet files\r\n",
					"parquet_directory = 'abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_OneYear.parquet/*'\r\n",
					"\r\n",
					"# Get a list of all partitioned Parquet files in the directory\r\n",
					"parquet_files = [os.path.join(parquet_directory, file) for file in os.listdir(parquet_directory) if file.endswith('.parquet')]\r\n",
					"\r\n",
					"# Read partitioned Parquet files into a single Table\r\n",
					"tables = [pq.read_table(parquet_file) for parquet_file in parquet_files]\r\n",
					"data_table = pq.concat_tables(tables)\r\n",
					"\r\n",
					"# Convert the Table to a Pandas DataFrame\r\n",
					"data = data_table.to_pandas()\r\n",
					"\r\n",
					"# Display the first few rows of the DataFrame\r\n",
					"print(data.head())\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# Replace 'your_directory' with the path to the directory containing the partitioned Parquet files\r\n",
					"parquet_directory = 'abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_OneYear.parquet'\r\n",
					"\r\n",
					"# Read the partitioned Parquet files into a single DataFrame\r\n",
					"data = pd.concat(\r\n",
					"    pd.read_parquet(file)\r\n",
					"    for file in pd.Series(parquet_directory).glob('*.parquet')\r\n",
					")\r\n",
					"\r\n",
					"# Display the first few rows of the DataFrame\r\n",
					"print(data.head())"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyarrow.parquet as pq\r\n",
					"import pandas as pd\r\n",
					"from glob import glob\r\n",
					"\r\n",
					"# Specify the path to the partitioned Parquet folder\r\n",
					"parquet_folder = \"abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_OneYear.parquet/*/\"\r\n",
					"\r\n",
					"# Use glob to get a list of all Parquet files in the specified folder\r\n",
					"parquet_files = glob(parquet_folder)\r\n",
					"print(parquet_files)\r\n",
					"# Initialize an empty list to hold the DataFrame chunks\r\n",
					"dataframes = []\r\n",
					"\r\n",
					"# Loop through each Parquet file and read it into a DataFrame\r\n",
					"for file in parquet_files:\r\n",
					"    table = pq.read_table(file)\r\n",
					"    dataframe = table.to_pandas()\r\n",
					"    dataframes.append(dataframe)\r\n",
					"print(dataframes)\r\n",
					"# Concatenate the DataFrames\r\n",
					"#data = pd.concat(dataframes, ignore_index=True)\r\n",
					"\r\n",
					"# Display the first few rows of the DataFrame\r\n",
					"#print(data.head())"
				],
				"execution_count": 21
			}
		]
	}
}