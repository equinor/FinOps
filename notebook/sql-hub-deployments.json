{
	"name": "sql-hub-deployments",
	"properties": {
		"folder": {
			"name": "NotebookInProduction/HUB"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "15c815f7-3d0e-438e-b5da-b19a99898a66"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Initialize script"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import timedelta, datetime\r\n",
					"from dateutil.relativedelta import relativedelta\r\n",
					"import calendar\r\n",
					"import json\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from azure.storage.blob import BlobServiceClient\r\n",
					"import pyspark.sql.functions as F\r\n",
					"import pyspark.sql.window as W\r\n",
					"from pyspark.sql import Row"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Constants\r\n",
					"HOURS_PER_YEAR = 8760\r\n",
					"KEY_VAULT_NAME = 'acm-toolkit-kv'\r\n",
					"LINKED_SERVICE_NAME = 'ACM_Toolkit_kv'\r\n",
					"hubAutomationConnectionString = mssparkutils.credentials.getSecret(KEY_VAULT_NAME , 'hubautomation-sa-connectionstring', LINKED_SERVICE_NAME)"
				],
				"execution_count": 36
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Lookup HUB SQL configuration"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"blob_service_client = BlobServiceClient.from_connection_string(hubAutomationConnectionString)\r\n",
					"\r\n",
					"# get a reference to the blob container and file of the SQL HUB configuration\r\n",
					"container_name = 'sql-config'\r\n",
					"blob_name = 'config.json'\r\n",
					"container_client = blob_service_client.get_container_client(container_name)\r\n",
					"blob_client = container_client.get_blob_client(blob_name)\r\n",
					"\r\n",
					"# download the blob content as a string\r\n",
					"blob_content = blob_client.download_blob().content_as_text()\r\n",
					"\r\n",
					"# parse the JSON string into a Python dictionary\r\n",
					"sql_config = json.loads(blob_content)\r\n",
					"\r\n",
					"# Compute the variable determining if results should be written to file\r\n",
					"day_name = datetime.now().strftime(\"%A\")\r\n",
					"should_run = sql_config['runDays'][day_name]\r\n",
					"\r\n",
					"sql_metersubcategory_array = sql_config['MeterSubCategory']\r\n",
					"sql_days_back_from = sql_config['daysBackFrom']\r\n",
					"sql_days_back_to = sql_config['daysBackTo']\r\n",
					"sql_enterprise_licence_cores = sql_config['enterprise_licence_cores']\r\n",
					"sql_standard_licence_cores = sql_config['enterprise_licence_cores']\r\n",
					"yearly_license_cost = sql_config['lic_cost_yearly']\r\n",
					"\r\n",
					"sql_normalized_licence_cores = (4 * sql_enterprise_licence_cores) + sql_standard_licence_cores\r\n",
					"\r\n",
					"# Hourly cost of license per vCPU\r\n",
					"hourly_vcpu_cost = yearly_license_cost / (HOURS_PER_YEAR * sql_normalized_licence_cores)"
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load and filter usage "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"columns_to_keep = [\r\n",
					"    'SubscriptionId',\r\n",
					"    'SubscriptionName',\r\n",
					"    'Date',\r\n",
					"    'ResourceGroup', \r\n",
					"    'ResourceName', \r\n",
					"    'ResourceId', \r\n",
					"    'MeterCategory', \r\n",
					"    'MeterSubCategory', \r\n",
					"    'MeterName',\r\n",
					"    'UnitOfMeasure',\r\n",
					"    'Quantity',\r\n",
					"    'UnitPrice',\r\n",
					"    'EffectivePrice',\r\n",
					"    'CostInBillingCurrency', \r\n",
					"    'ServiceInfo2',\r\n",
					"    'PartNumber',\r\n",
					"    'ProductName', \r\n",
					"    'ai_VCPUs',\r\n",
					"    'ai_AHB',\r\n",
					"    'LicensePayGUnitPrice'\r\n",
					"]\r\n",
					"\r\n",
					"sql_columns = columns_to_keep + [\r\n",
					"    'SQLAHB',\r\n",
					"    'SQLAHB_VCPUs',\r\n",
					"    'ai_vCores'\r\n",
					"]"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cost_path = f'abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyActualCost/*/Extended_v3_ACMMonthlyActualCost_*.parquet'\r\n",
					"cost_df = spark.read.format('parquet').load(cost_path)"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Only select usage from the period specified in the configuration file\r\n",
					"sql_start_date = (datetime.now() - timedelta(days=sql_days_back_from)).strftime('%Y-%m-%d')\r\n",
					"sql_end_date = (datetime.now() - timedelta(days=sql_days_back_to)).strftime('%Y-%m-%d')\r\n",
					"cost_df = cost_df.where((F.col('Date') >= sql_start_date) & (F.col('Date') <= sql_end_date))"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cost_df = cost_df.select(*sql_columns)\r\n",
					"cost_df = cost_df.where(F.col('MeterSubCategory').isin(sql_metersubcategory_array))\r\n",
					"cost_df = cost_df.where(~F.upper(F.col('ResourceName')).startswith('CVD-'))\r\n",
					"\r\n",
					"# Create new date column\r\n",
					"cost_df = cost_df.withColumn('Date', F.current_date())\r\n",
					"\r\n",
					"# Copy df for disabled list computation\r\n",
					"cost_copy_df = cost_df.alias('cost_copy_df')"
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Compute set of SQL instances getting HUB enabled"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Max aggregation on vCores are used to mitigate duplicate instances, as they can occur if # of vCores are altered on a given day\r\n",
					"sql_enable_df = cost_df \\\r\n",
					"    .select('Date', 'ProductName', 'ResourceId', 'SubscriptionId', 'ResourceGroup', 'ResourceName', 'Quantity', 'ai_VCPUs', 'ai_vCores', 'SQLAHB_VCPUs','LicensePayGUnitPrice') \\\r\n",
					"    .groupBy('Date', 'ProductName', 'ResourceId', 'SubscriptionId', 'ResourceGroup', 'ResourceName', 'ai_VCPUs', 'LicensePayGUnitPrice') \\\r\n",
					"    .agg(F.sum('Quantity').alias('ResourceHours'), F.max('ai_vCores').alias('vCores'), F.max('SQLAHB_VCPUs').alias('SQLAHB_VCPUs'))\r\n",
					"\r\n",
					"sql_enable_df = sql_enable_df.withColumn('PAYGLicenseCost', F.col('ResourceHours') * F.col('LicensePayGUnitPrice'))\r\n",
					"\r\n",
					"# Compute hourly HUB license cost \r\n",
					"sql_enable_df = sql_enable_df.withColumn('HUBLicenseCost', F.col('ResourceHours') * F.col('SQLAHB_VCPUs') * hourly_vcpu_cost)\r\n",
					"\r\n",
					"# Compute savings per VCPU\r\n",
					"sql_enable_df = sql_enable_df.withColumn('HUBSavingsPerVCPU', (F.col('PAYGLicenseCost') - F.col('HUBLicenseCost')) / F.col('SQLAHB_VCPUs'))\r\n",
					"\r\n",
					"# Sort resources according to which we want to enable HUB for first\r\n",
					"sql_enable_df = sql_enable_df.orderBy(F.desc('HUBSavingsPerVCPU'))\r\n",
					"\r\n",
					"window = W.Window.rowsBetween(W.Window.unboundedPreceding, 0)\r\n",
					"sql_enable_df = sql_enable_df.withColumn(\"TotalNormalizedCores\", F.sum(\"SQLAHB_VCPUs\").over(window))\r\n",
					"\r\n",
					"# Only include instances that doesnt overshoot the number of available license cores\r\n",
					"sql_enable_df = sql_enable_df.where(F.col('TotalNormalizedCores') <= sql_normalized_licence_cores)\r\n",
					"\r\n",
					"# Persist copy for activity log computation\r\n",
					"enable_copy_df = sql_enable_df.alias('enable_copy_df')\r\n",
					"\r\n",
					"# Remove redundant columns from results\r\n",
					"sql_enable_df = sql_enable_df.select('Date', 'ResourceId', 'SubscriptionId', 'ResourceName', 'ResourceGroup', 'SQLAHB_VCPUs')"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Compute set of SQL instances getting HUB disabled"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Compute list of disabled instances by removing references from the enabled liste\r\n",
					"sql_disable_df = cost_copy_df.where(F.col('SQLAHB') == 'Enabled')\r\n",
					"sql_disable_df = sql_disable_df.select('Date', 'ResourceId', 'SubscriptionId', 'ResourceName', 'ResourceGroup', 'SQLAHB_VCPUs')\r\n",
					"sql_disable_df = sql_disable_df.dropDuplicates([\"ResourceId\"])\r\n",
					"sql_disable_df = sql_disable_df.join(sql_enable_df, 'ResourceId', 'left_anti')"
				],
				"execution_count": 43
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Store HUB deployment results and update activity log"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def compute_activity_log_entry(activity_path):\r\n",
					"    csv_options = {'header' : True,\r\n",
					"                'delimiter' : ',',\r\n",
					"                'quote' : '\"',\r\n",
					"                'escape' : '\"'}\r\n",
					"    activity_df = spark.read.options(**csv_options).csv(activity_path)\r\n",
					"\r\n",
					"    # Compute the number of cores ACTUALLY assigned\r\n",
					"    sql_assigned_cores = enable_copy_df.agg(F.max('TotalNormalizedCores')).collect()[0][0]\r\n",
					"\r\n",
					"    # Aggregate normalized core per priority level \r\n",
					"    normalized_cores_by_priority = enable_copy_df.select('SQLAHB_VCPUs', 'Priority').groupBy('Priority').agg(F.sum('SQLAHB_VCPUs').alias('TotalCores')).collect()\r\n",
					"\r\n",
					"    # Loop through aggregations and persist values in array \r\n",
					"    activity_values = [0, 0, 0, 0, 0, 0]\r\n",
					"    index = 0\r\n",
					"    for value in normalized_cores_by_priority:\r\n",
					"        activity_values[index] = value['TotalCores']\r\n",
					"        index+=1\r\n",
					"\r\n",
					"    today = (datetime.now().strftime('%Y-%m-%d'))\r\n",
					"    new_activity_row = spark.createDataFrame([(today, *activity_values, sql_normalized_licence_cores, sql_assigned_cores)], activity_df.columns)\r\n",
					"    activity_df = activity_df.union(new_activity_row)\r\n",
					"\r\n",
					"    return activity_df"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"enable_path = 'abfss://sql-hub-logs-v2@hubautomation.dfs.core.windows.net/LATEST-AHUB-Deployment.csv'\r\n",
					"disable_path = 'abfss://sql-hub-logs-v2@hubautomation.dfs.core.windows.net/LATEST-AHUB-Removal.csv'\r\n",
					"activity_path = 'abfss://sql-activity-v2@hubautomation.dfs.core.windows.net/activity.csv'\r\n",
					"\r\n",
					"print(f\"Should run? {should_run}\")\r\n",
					"\r\n",
					"if should_run:\r\n",
					"    # Store enabled list in storage account\r\n",
					"    print(\"Writing enabled list to SQL latest path\")\r\n",
					"    sql_enable_df.toPandas().to_csv(enable_path, index=False)\r\n",
					"\r\n",
					"    print(\"Writing disabled list to SQL latest path\")\r\n",
					"    # Store disabled list in storage account\r\n",
					"    sql_disable_df.toPandas().to_csv(disable_path, index=False)\r\n",
					"\r\n",
					"    # Compute activity log entry and write back to file\r\n",
					"    print(\"Updating SQL activity log\")\r\n",
					"    activity_df = compute_activity_log_entry(activity_path)\r\n",
					"    activity_df.toPandas().to_csv(activity_path, index=False)"
				],
				"execution_count": 81
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Output 'should_run' indication when running in pipeline\r\n",
					"mssparkutils.notebook.exit(should_run)"
				],
				"execution_count": 82
			}
		]
	}
}