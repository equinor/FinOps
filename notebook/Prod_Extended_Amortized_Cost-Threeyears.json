{
	"name": "Prod_Extended_Amortized_Cost-Threeyears",
	"properties": {
		"description": "\n",
		"folder": {
			"name": "NotebookInProduction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "cdcd4ea1-aacb-43d0-962c-44ab55800aca"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This notebook is used to extract all extended amortized cost files from storage account as dataframes and append them into one single dataframe.\r\n",
					"\r\n",
					"The purpose is to make PBI desktop convenient and faster to load the data. \r\n",
					"\r\n",
					"The appended data is considered as a large dataset with more than a hundred million rows and multiple columns. \r\n",
					"\r\n",
					"To reduce the size of the data, it has been aggregated to limited columns and a three years period have been selected. This has been verified and confirmed by the end-users (WBS owners).\r\n",
					"\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import Libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd \r\n",
					"import pyspark.pandas as ps\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.types import StructType\r\n",
					"from pyspark.sql.functions import lit\r\n",
					"from pyspark.sql.functions import year, month\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from datetime import datetime, timedelta\r\n",
					"from pyspark.sql.functions import when\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load the data"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Collect the data schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Identify a three years period\r\n",
					"date_three_years_ago = datetime.now().date()-timedelta(days=1095)\r\n",
					"\r\n",
					"# Define a path for the data with random period\r\n",
					"path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost/20210101-20210131/Extended_v3_ACMMonthlyAmortizedCost_20210101-20210131.parquet\"\r\n",
					"\r\n",
					"# Load the data schema for later use\r\n",
					"df_schema_extended = spark.read.format('parquet').load(path)\r\n",
					"\r\n",
					"# Add new empty date column for later use\r\n",
					"df_schema_extended = df_schema_extended.withColumn('Report_Date',lit('') )\r\n",
					"df_schema_extended = df_schema_extended.withColumn('Report_Date_Partition',lit('') )"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(\"Three-years-ago's date since today:\",date_three_years_ago)\r\n",
					"len(df_schema_extended.columns)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# In case there is a need to change the column name\r\n",
					"#df_schema_extended_pd = df_schema_extended_pd.rename(columns={'Azure Hybrid Benefit': 'Azure_Hybrid_Benefit'})\r\n",
					"#df_schema_extended = spark.createDataFrame(df_schema_extended_pd)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load all ACMMonthlyActualCost data and append to one dataframe."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Create a SparkSession\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"\r\n",
					"# Define an empty schema\r\n",
					"schema = df_schema_extended.schema\r\n",
					"\r\n",
					"# Create an empty DataFrame with the specified schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the root path\r\n",
					"root_path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost\"\r\n",
					"\r\n",
					"# Collect all the folders under this root path\r\n",
					"folders = mssparkutils.fs.ls(root_path)\r\n",
					"\r\n",
					"\r\n",
					"for folder in folders:\r\n",
					"    # Collect all file path from the folder\r\n",
					"        datetime_str = folder.name[0:8]\r\n",
					"        datetime_year = int(datetime_str[0:4])\r\n",
					"        datetime_month = int(datetime_str[4:6])\r\n",
					"        datetime_day = int(datetime_str[6:8])\r\n",
					"        timestamp = pd.Timestamp(year=datetime_year, month=datetime_month, day=datetime_day).date()\r\n",
					"        if timestamp > date_three_years_ago:           \r\n",
					"            file_paths = mssparkutils.fs.ls(root_path +'/'+ str(folder.name))\r\n",
					"            for file_path in file_paths:\r\n",
					"                # Focus on parquet file not csv file\r\n",
					"                if 'Extended_v3' in file_path.name:\r\n",
					"                    try:\r\n",
					"                        # Add a Report date column in the dataframe\r\n",
					"                        date = pd.to_datetime(folder.path.split('/')[-1].split('-')[0],format=\"%Y/%m/%d\")\r\n",
					"                        df_temp = spark.read.format('parquet').load(file_path.path)\r\n",
					"                        print(date)\r\n",
					"                        df_temp = df_temp.withColumn('Report_Date', lit(date))\r\n",
					"                        df_temp = df_temp.withColumn('Report_Date_Partition', lit(date))\r\n",
					"                        # Append the dataframe\r\n",
					"                        df = df.union(df_temp)\r\n",
					"                    except Exception as e:\r\n",
					"                        # Exclude the extended files that are developed by David.\r\n",
					"                        print(\"Those files will not be read\",file_path.path)\r\n",
					"                        print(df_temp.columns)\r\n",
					"                        print(df)\r\n",
					"                        print(len(df_temp.columns),len(df.columns))"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Display the dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Change the column name\r\n",
					"df = df.withColumnRenamed(\"Application_Name\", \"ApplicationName\")\r\n",
					"\r\n",
					"# Applied Unknown label in ApplicationName field. \r\n",
					"condition_1 = (F.col('ApplicationName').isNull()| (F.col('ApplicationName') == \"\")) & (F.col('SubscriptionName').contains('Danske Commodities'))\r\n",
					"condition_2 = (F.col('ApplicationName').isNull()| (F.col('ApplicationName') == \"\")) & (~(F.col('SubscriptionName').contains('Danske Commodities')))\r\n",
					"\r\n",
					"condition_1 = (F.col('ApplicationName') == 'Application not defined or not found') & (F.col('SubscriptionName').contains('Danske Commodities'))\r\n",
					"\r\n",
					"df = df.withColumn(\r\n",
					"    'ApplicationName',\r\n",
					"    F.when(condition_1,'Danske Commodities'\r\n",
					"    ).when(condition_2,'Unknown'\r\n",
					"    ).otherwise(F.col('ApplicationName'))\r\n",
					")\r\n",
					"\r\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#print('Number of rows in the appended dataframe:',df.count())\r\n",
					"#display(df)\r\n",
					"#display(df.filter((F.col('ApplicationName').isNull())|(F.col('ApplicationName')=='')))\r\n",
					"#display(df.filter((F.col('ApplicationName')).contains('Danske Commodities')))\r\n",
					"#display(df.filter((F.col('ApplicationName')).contains('Unknown')))"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Aggregate the data \r\n",
					"Reduce the size of the data to fit PowerBI performance\r\n",
					"\r\n",
					"Only select the useful fields"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Aggregate the data\r\n",
					"df_agg = df.groupBy('SubscriptionId','ApplicationName','SubscriptionName','CostCenter','ResourceGroup','ResourceLocation','ResourceName','MeterName','MeterCategory','MeterSubCategory','Report_Date','Report_Date_Partition','SubscriptionServiceNow-App','ActiveWBS','CostAllocationType','AppID','Date').agg(F.sum('Quantity').alias('TotalQuantity'),F.sum('CostInBillingCurrency').alias('CostInKroner'))"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Change the data type of the 'Report_Date_Partition' column to Datetime\r\n",
					"df_agg = df_agg.withColumn(\"Report_Date_Partition\", col(\"Report_Date_Partition\").cast(\"timestamp\"))"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Split the date column into year and month columns for partition purpose\r\n",
					"df_agg = df_agg.withColumn(\"year_partition\", year(df_agg[\"Report_Date_Partition\"]))\r\n",
					"df_agg = df_agg.withColumn(\"month_partition\", month(df_agg[\"Report_Date_Partition\"]))"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Display aggregated data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Change the data type of the 'Report_Date' column to Datetime\r\n",
					"df_agg = df_agg.withColumn(\"Report_Date\", col(\"Report_Date\").cast(\"timestamp\"))\r\n",
					"print(\"The number of the rows in the aggregated dataframe:\", df_agg.count())"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Select the columns needed for visualization\r\n",
					"selected_columns = ['SubscriptionId','ApplicationName','SubscriptionName','CostCenter','ResourceGroup','ResourceLocation','ResourceName','MeterName','MeterCategory','MeterSubCategory', \r\n",
					" 'Report_Date',\r\n",
					" 'TotalQuantity',\r\n",
					" 'CostInKroner',\r\n",
					" 'year_partition',\r\n",
					" 'month_partition','SubscriptionServiceNow-App','ActiveWBS','CostAllocationType','AppID','Date']\r\n",
					"df_agg = df_agg.select(*selected_columns)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Verify the result\r\n",
					"df_agg.groupBy(\"ApplicationName\").count().orderBy(\"count\", ascending=False).show()"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#df_agg.printSchema()\r\n",
					"#print(\"The number of the rows in the aggregated dataframe:\", df_agg.count())\r\n",
					"#display(df_agg)\r\n",
					"# display(df_agg)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Save in optimized container"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define optimized path to save the data\r\n",
					"optimized_path = f\"abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_Threeyears.parquet\" \r\n",
					"\r\n",
					"# Save the data with partitioned by Month and Year.\r\n",
					"df_agg.write.format('parquet').mode('overwrite').partitionBy('year_partition','month_partition').option('overwriteSchema', 'true').save(optimized_path)\r\n",
					"\r\n",
					"# Save the data with no partition.\r\n",
					"#df_agg.write.format('parquet').mode('overwrite').partitionBy('Report_date).option('overwriteSchema', 'true').save(optimized_path)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Clear cache in Spark session"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"currentSparkSession = SparkSession.builder.getOrCreate()\r\n",
					"spark.catalog.clearCache()"
				],
				"execution_count": 17
			}
		]
	}
}