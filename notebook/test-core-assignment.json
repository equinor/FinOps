{
	"name": "test-core-assignment",
	"properties": {
		"folder": {
			"name": "NotebookInProduction/HUB and RI Savings"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "7656b2cb-141f-4998-bb75-9184ed805854"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Calculate over/under assignment of HUB\r\n",
					"1. Compute # of resources with HUB and # of VCPUs on 12th of March (pre) **317** (1817)\r\n",
					"2. Compute # of resources with HUB and # of VCPUs on 14th of March (post) **293** (2481)\r\n",
					"3. Compute # of resources with HUB enabled and # of VCPUs on 13th of March (added) **313** (1341)\r\n",
					"\r\n",
					"- **11** of these resources does not exist any longer, and deployment fails for these. These account for **80** normalized cores\r\n",
					"\r\n",
					"4. Compute # of resources with HUB disabled and # of VCPUs on 13th of March (removed) **50** (528)\r\n",
					"5. How many resources/vCPUs was included kept from 12th to 14th (kept)? **243** (1289)\r\n",
					"\r\n",
					"- **9** of the resources marked as _HUB Enabled_ is not reflected in the normalized core column. These are **Azure Arc-Enabled SQL Server** instances and amount to **184** normalized cores (but are not reflected in the extend files)\r\n",
					" \r\n",
					"6. How many resources/vCPUs was similar on the 12th and the enabled list? (1199)\r\n",
					"7. How many resources/vCPUs was similar on the 14th and the enabled list?\r\n",
					"8. How many resources in the disabled list was enabled on the 12th? (should be all)\r\n",
					"9. How many resources in the disabled list was enabled on the 14th? (should be none)\r\n",
					"10. How many resources on the enable list was not enabled on the 14th?\r\n",
					"11. How many enabled resources on the 14th was not in the enable list?\r\n",
					"12. How many resources was failed to enabled/diabled on the 13th?\r\n",
					"\r\n",
					"\r\n",
					"- Are more/less normalized cores being enabled than available licenses?\r\n",
					"- Get list of resources not being enabled that should be enabled\r\n",
					"- Get list of resources that are enabled but are not in the enabled list\r\n",
					"- Do the same for Windows HUB"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"add_w_vcpu = add_df.join(cost_today, 'ResourceId', 'left')\r\n",
					"rm_w_vcpu = rm_df.join(cost_today, 'ResourceId', 'left')"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(rm_w_vcpu.where((F.col('SQLAHB_VCPUS') == 0) | F.col('SQLAHB_VCPUS').isNull()))"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(cost_pre.where((F.col('ResourceId').isin(pre.intersection(added))) & (F.col('SQLAHB') == 'Enabled')).dropDuplicates(['ResourceId']).count())"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as F"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"source": [
					"test_path = 'abfss://sql-hub-logs@hubautomation.dfs.core.windows.net/2024-03-13-AHUB-Deployment.csv'\r\n",
					"csv_options = {'header' : True,\r\n",
					"                'delimiter' : ',',\r\n",
					"                'quote' : '\"',\r\n",
					"                'escape' : '\"'}\r\n",
					"add_df = spark.read.options(**csv_options).csv(test_path)\r\n",
					"\r\n",
					"add_ids = [row[0] for row in add_df.select(\"ResourceId\").collect()]\r\n",
					"\r\n",
					"test_path = 'abfss://sql-hub-logs@hubautomation.dfs.core.windows.net/2024-03-13-AHUB-Removal.csv'\r\n",
					"csv_options = {'header' : True,\r\n",
					"                'delimiter' : ',',\r\n",
					"                'quote' : '\"',\r\n",
					"                'escape' : '\"'}\r\n",
					"rm_df = spark.read.options(**csv_options).csv(test_path)\r\n",
					"\r\n",
					"rm_ids = [row[0] for row in rm_df.select(\"ResourceId\").collect()]\r\n",
					"\r\n",
					"cost_path = monthly_path = f'abfss://usage@{storageAccount}.dfs.core.windows.net/exports/monthly/ACMMonthlyActualCost/*/Extended_v3_ACMMonthlyActualCost_*.parquet'\r\n",
					"test_cost = spark.read.format('parquet').load(cost_path)\r\n",
					"\r\n",
					"cost_pre = test_cost.where(F.col('Date') == '2024-03-12')\r\n",
					"cost_pre = cost_pre.where(F.col('SQLAHB') == 'Enabled')\r\n",
					"\r\n",
					"cost_today = test_cost.where(F.col('Date') >= '2024-03-01')\r\n",
					"cost_today = cost_today.where(F.col('SQLAHB') == 'Enabled')\r\n",
					"\r\n",
					"cost_post = test_cost.where(F.col('Date') == '2024-03-14')\r\n",
					"cost_post = cost_post.where(F.col('SQLAHB') == 'Enabled')\r\n",
					"\r\n",
					"pre_ids = [row[0] for row in cost_pre.dropDuplicates(['ResourceId']).select(\"ResourceId\").collect()]\r\n",
					"post_ids = [row[0] for row in cost_post.dropDuplicates(['ResourceId']).select(\"ResourceId\").collect()]\r\n",
					"\r\n",
					"pre = set(pre_ids)\r\n",
					"post = set(post_ids)\r\n",
					"added = set(add_ids)\r\n",
					"removed = set(rm_ids)\r\n",
					"\r\n",
					"kept = pre.intersection(post)\r\n",
					"diff = added.intersection(pre)\r\n",
					"# display(cost_post.agg(F.sum('SQLAHB_VCPUs')))\r\n",
					"# display(cost_post.where(F.col('ResourceId').isin(kept)).orderBy(F.desc('SQLAHB_VCPUs')))\r\n",
					"# display(add_df.join(cost_post, 'ResourceId', 'left').dropDuplicates(['Resourceid']).agg(F.sum('SQLAHB_VCPUs')))\r\n",
					""
				],
				"execution_count": 54
			}
		]
	}
}