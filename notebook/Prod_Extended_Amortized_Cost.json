{
	"name": "Prod_Extended_Amortized_Cost",
	"properties": {
		"description": "\n",
		"folder": {
			"name": "tsc/Unused"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "eed54a4d-e702-486a-9d19-da45fcb4aac4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sparkpool32",
				"name": "sparkpool32",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Install older version of pandas to avoid any mismatch issue"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import Libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd \r\n",
					"import pyspark.pandas as ps\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import StructType\r\n",
					"import pandas as pd\r\n",
					"from pyspark.sql.functions import lit"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load the data"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Collect the data schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define a path for the data with random period\r\n",
					"path = \"abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost/20210101-20210131/Extended_ACMMonthlyAmortizedCost_20210101-20210131.parquet\"\r\n",
					"\r\n",
					"# Load the data schema and add a new column for Report Date\r\n",
					"df_schema_extended = spark.read.format('parquet').load(path)\r\n",
					"#df_schema_extended_pd = df_schema_extended_pd.rename(columns={'Azure Hybrid Benefit': 'Azure_Hybrid_Benefit'})\r\n",
					"#df_schema_extended = spark.createDataFrame(df_schema_extended_pd)\r\n",
					"df_schema_extended = df_schema_extended.withColumn('Report_Date',lit('') )\r\n",
					"df_schema_extended = df_schema_extended.withColumn('Report_Date_Partition',lit('') )"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"len(df_schema_extended.columns)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load all ACMMonthlyActualCost data and append to one dataframe."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Create a SparkSession\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"\r\n",
					"# Define an empty schema\r\n",
					"schema = df_schema_extended.schema\r\n",
					"\r\n",
					"# Create an empty DataFrame with the specified schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the root path\r\n",
					"root_path = \"abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/ACMMonthlyAmortizedCost\"\r\n",
					"\r\n",
					"# Collect all the folders under this root path\r\n",
					"folders = mssparkutils.fs.ls(root_path)\r\n",
					"\r\n",
					"\r\n",
					"for folder in folders:\r\n",
					"    # Collect all file path from the folder\r\n",
					"    file_paths = mssparkutils.fs.ls(root_path +'/'+ str(folder.name))\r\n",
					"    for file_path in file_paths:\r\n",
					"        # Focus on parquet file not csv file\r\n",
					"        if 'Extended_' in file_path.name:\r\n",
					"            try:\r\n",
					"                # Add a Report date column in the dataframe\r\n",
					"                date = pd.to_datetime(folder.path.split('/')[-1].split('-')[0],format=\"%Y/%m/%d\")\r\n",
					"                df_temp = spark.read.format('parquet').load(path)\r\n",
					"                df_temp = df_temp.withColumn('Report_Date', lit(date))\r\n",
					"                df_temp = df_temp.withColumn('Report_Date_Partition', lit(date))\r\n",
					"                # Append the dataframe\r\n",
					"                df = df.union(df_temp)\r\n",
					"            except Exception as e:\r\n",
					"                # Exclude the extended files that are developed by David.\r\n",
					"                print(\"Those files will not be read\",file_path.path)\r\n",
					""
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Display the dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"print('Number of rows:',df.count())\r\n",
					"display(df)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Aggregate the data \r\n",
					"Reduce the size of the data to fit PowerBI performance\r\n",
					"\r\n",
					"Only select the useful fields"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Aggregate the data\r\n",
					"df_agg = df.groupBy('SubscriptionId','SubscriptionName','CostCenter','ResourceGroup','ResourceLocation','ResourceName','MeterName','MeterCategory','MeterSubCategory','Report_Date','Report_Date_Partition').agg(F.sum('Quantity').alias('TotalQuantity'),F.sum('CostInBillingCurrency').alias('CostInKroner'))"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Display aggregated data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"print(\"The number of the rows:\", df_agg.count())\r\n",
					"display(df_agg)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Save it to optimized container"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define optimized path to save the data\r\n",
					"optimized_path = \"abfss://usage@s037costmgmt.dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_Aggregated_overview.parquet\" \r\n",
					"\r\n",
					"#Save the data with partitioned by time\r\n",
					"df_agg.write.format('parquet').mode('overwrite').partitionBy('Report_Date').option('overwriteSchema', 'true').save(optimized_path)"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Clear cache in Spark session"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"currentSparkSession = SparkSession.builder.getOrCreate()\r\n",
					"spark.catalog.clearCache()"
				],
				"execution_count": 2
			}
		]
	}
}