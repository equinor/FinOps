{
	"name": "Anomaly_Detection_v2",
	"properties": {
		"folder": {
			"name": "NotebookInProduction"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sprkpool33large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "d9741362-012a-4185-a791-8ff2eaee35d8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/13d66f54-0a19-4912-b4f3-54d15897368d/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/s037-cost-management/bigDataPools/sprkpool33large",
				"name": "sprkpool33large",
				"type": "Spark",
				"endpoint": "https://s037-cost-management.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sprkpool33large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Pipeline parameter\r\n",
					"storageAccount = 's037costmgmt'"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"import pyspark.sql.types as T\r\n",
					"from pyspark.sql.window import Window"
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def preprocessing_anomaly_detection(df):\r\n",
					"\r\n",
					"    # Selecting just relevant columns for anomaly detection\r\n",
					"    new_df = df.select('SubscriptionName', 'ResourceGroup', 'Date', 'CostInBillingCurrency')\r\n",
					"\r\n",
					"    # Filtering out data older than 90 days to get only the latest historical data\r\n",
					"    threshold = F.date_sub(F.current_date(), 90)\r\n",
					"    filtered_df = new_df.filter(F.col('Date') >= threshold)\r\n",
					"\r\n",
					"    # Grouping the data by each filtering level and getting the sum of all costs related to the filtering columns\r\n",
					"    grouped_df = filtered_df.groupBy('SubscriptionName',\r\n",
					"                                     'ResourceGroup',\r\n",
					"                                     'Date').agg(F.round(\r\n",
					"                                                  F.sum(\r\n",
					"                                                   F.col('CostInBillingCurrency')), 1).alias('TotalCost'))\r\n",
					"\r\n",
					"    return grouped_df"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def anomaly_detection_classifier(df,\r\n",
					"                                 filter_column_1,\r\n",
					"                                 filter_column_2,\r\n",
					"                                 window_size=30):\r\n",
					"\r\n",
					"    # Setting a partition window to divide the dataset in the filter options and order it by the Date\r\n",
					"    windowSpec = Window.partitionBy(filter_column_1, filter_column_2).orderBy('Date').rowsBetween(-window_size, 0)\r\n",
					"\r\n",
					"    # Set a moving average that is set by the n previous days\r\n",
					"    costs_by_dates = df.withColumn('MovingAverage', F.round(F.avg(F.col('TotalCost')).over(windowSpec), 1))\r\n",
					"\r\n",
					"    # Set a moving standard deviation of the same period\r\n",
					"    costs_by_dates = costs_by_dates.withColumn('MovingStdDev', F.round(F.stddev(F.col('TotalCost')).over(windowSpec), 1))\r\n",
					"\r\n",
					"    # Setting a scaling mechanism on the threshold in order to reduce the threshold when the costs are very high\r\n",
					"    am_curve = lambda x: 13.6 * x ** -0.22\r\n",
					"    \r\n",
					"    # Set a minimum scale of 0.1 and a maximum scale of 3.5\r\n",
					"    costs_by_dates = costs_by_dates.withColumn('Scaler', F.when(F.col('MovingStdDev') > 0.1, am_curve(F.col('MovingStdDev'))).otherwise(F.lit(0.1)))\r\n",
					"    costs_by_dates = costs_by_dates.withColumn('Scaler', F.when(F.col('Scaler') > 3.5, F.lit(3.5)).otherwise(F.col('Scaler')))\r\n",
					"\r\n",
					"    costs_by_dates = costs_by_dates.withColumn('Threshold', F.col('MovingAverage') + (F.col('MovingStdDev') * F.col('Scaler')))\r\n",
					"\r\n",
					"     # If the cost is more than the stddev threshold (+ some noise cancellation), then set the datapoint to be 1, otherwise 0\r\n",
					"    costs_by_dates = costs_by_dates.withColumn('AboveThreshold', F.when(F.col('TotalCost') > (F.col('Threshold') + 1), F.lit(1))\\\r\n",
					"                                                                                 .otherwise(F.lit(0)))\r\n",
					"    return costs_by_dates"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def anomaly_notification_filtration(df,\r\n",
					"                                    filter_column_1,\r\n",
					"                                    filter_column_2):\r\n",
					"\r\n",
					"    # Only consider cost items that is above threshold\r\n",
					"    thres_df = df.filter(F.col('AboveThreshold') == 1)\r\n",
					"\r\n",
					"    # Set rolling window to capture date of previous anomaly in resource group\r\n",
					"    window_spec = Window.partitionBy('SubscriptionName', 'ResourceGroup').orderBy('Date')\r\n",
					"    thres_df = thres_df.withColumn('PrevDate', F.lag('Date').over(window_spec))\r\n",
					"\r\n",
					"    # Compute number of days between anomalies in resource group\r\n",
					"    thres_df = thres_df.withColumn('DateDiff', F.datediff(F.col('Date'), F.col('PrevDate')))\r\n",
					"\r\n",
					"    # Notify anomaly if anomalies haven't occured on subsequent days\r\n",
					"    notify_mask = (F.col('DateDiff').isNull()) | (F.col('DateDiff') > 1)\r\n",
					"    thres_df = thres_df.withColumn('NotifyAnomaly', F.when( notify_mask, F.lit(True)).otherwise(F.lit(False)))\r\n",
					"\r\n",
					"    # Only use cost items exceeding 10.000 NOK\r\n",
					"    thres_df = thres_df.filter(F.col('TotalCost') >= 10000)\r\n",
					"\r\n",
					"    # Dropping insignificant columns\r\n",
					"    dropped_df = thres_df.drop('Scaler', 'PrevDate', 'DateDiff')\r\n",
					"\r\n",
					"    # Filtering only the relevant data\r\n",
					"    filtered_df = dropped_df.filter(F.col('NotifyAnomaly') == True)\r\n",
					"\r\n",
					"    return filtered_df"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"def merge_with_subscription(input_df, subscription_path):\r\n",
					"\r\n",
					"    # Reading in useful columns from the subscription.json file\r\n",
					"    sub_df = spark.read.json(subscription_path).select('name', 'technicalOwner')\r\n",
					"\r\n",
					"    # Merging on subscription names\r\n",
					"    merged_df = input_df.join(sub_df, on=input_df.SubscriptionName == sub_df.name, how='left')\r\n",
					"    \r\n",
					"    # Dropping the right-join column to avoid duplicate records\r\n",
					"    merged_df = merged_df.drop('name')\r\n",
					"\r\n",
					"    return merged_df"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"source_path = \"abfss://usage@\" + storageAccount + \".dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_overview_OneYear.parquet\"\r\n",
					"destination_path = \"abfss://usage@\" + storageAccount + \".dfs.core.windows.net/exports/monthly/aggregate/parquet/Extended_ACMMonthlyAmortizedCost_AnomalyDetection_3mnth.parquet\"\r\n",
					"subscription_path = \"abfss://usage@\" + storageAccount + \".dfs.core.windows.net/subscriptions/subscriptions_2023-08-31.json\"\r\n",
					"\r\n",
					"print(f\"Reading data from: {source_path}\")\r\n",
					"df = spark.read.format('parquet').load(source_path)\r\n",
					"\r\n",
					"preproc_data = preprocessing_anomaly_detection(df)\r\n",
					"\r\n",
					"\r\n",
					"filter_1 = 'SubscriptionName'\r\n",
					"filter_2 = 'ResourceGroup'\r\n",
					"\r\n",
					"\r\n",
					"print(\"Running the Anomaly Detection Classifier for Subscription-partitioned data\")\r\n",
					"anomaly_df =  anomaly_detection_classifier(df=preproc_data,\r\n",
					"                                           filter_column_1=filter_1,\r\n",
					"                                           filter_column_2=filter_2,\r\n",
					"                                           window_size=60)\r\n",
					"\r\n",
					"\r\n",
					"filtered_df = anomaly_notification_filtration(anomaly_df,\r\n",
					"                                              filter_column_1=filter_1,\r\n",
					"                                              filter_column_2=filter_2)\r\n",
					"if filtered_df.count() == 0:\r\n",
					"    print('No Anomalies today!')\r\n",
					"else:\r\n",
					"    merged_df = merge_with_subscription(filtered_df, \r\n",
					"                                        subscription_path)\r\n",
					"\r\n",
					"\r\n",
					"    print(f\"Writing classification data to this path: {destination_path}\")\r\n",
					"    merged_df.write.format('parquet').mode('overwrite').save(destination_path)\r\n",
					"    print('Write Complete!')"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# display(anomaly_df.filter((F.col('SubscriptionName') == 'S034-Geology-Geophysics') & (F.col('ResourceGroup') == 'S034-WE-CC-1')))"
				],
				"execution_count": 60
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Finding Hyper-parameters for the Scaling-Function"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import numpy as np\r\n",
					"# from scipy.optimize import curve_fit\r\n",
					"# import matplotlib.pyplot as plt\r\n",
					"\r\n",
					"# # The function we want to use to apply the scaling function \r\n",
					"# def scaling_function(x, a, b):\r\n",
					"#     return a * np.power(x, b)\r\n",
					"\r\n",
					"# # Costs are the \"input values\" and the degree of scaling are the output values\r\n",
					"# input_values = np.array([1000, 10000, 100000])\r\n",
					"# output_values = np.array([3, 2, 1])\r\n",
					"\r\n",
					"# # Finding the unknown parameters (a, b) for the scaling function\r\n",
					"# params, covariance = curve_fit(scaling_function, input_values, output_values)\r\n",
					"\r\n",
					"# print(f\"'a' has value: {params[0]}\")\r\n",
					"# print(f\"'b' has value: {params[1]}\")\r\n",
					"\r\n",
					"# # Plotting the scaling function against a range\r\n",
					"# x = range(0, 100000, 1000)\r\n",
					"# y = [scaling_function(i, 13.6, -0.22) for i in x]\r\n",
					"# plt.figure(figsize=(14, 10))\r\n",
					"# plt.plot(x, y, c='blue', label='Fitted curve')\r\n",
					"# plt.scatter(input_values, output_values, c='red', label='Input data points')\r\n",
					"# plt.legend()\r\n",
					"# plt.show()"
				],
				"execution_count": 59
			}
		]
	}
}